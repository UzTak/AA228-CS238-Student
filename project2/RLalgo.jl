using Printf
using DataFrames
using CSV
using LinearAlgebra


struct MDP 
    Î³   # discount rate 
    S   # state space
    A   # action space
    T   # transition function
    R   # reward function
    TR  # transition function 
end 


""" Update scheme """

# Incremental Estimate (expectation)
mutable struct IncrementalEstimate
    Î¼  # mean estimate 
    Î±  # learning rate (function) 
    m  # num of updates 
end 

function update!(model::IncrementalEstimate, x)
    model.m += 1 
    model.Î¼ += model.Î±(model.m) * (x - model.Î¼) 
    return model 
end 

# Q-learning 
mutable struct QLearning
    S 
    A
    Î³
    Q
    Î±
end 

lookahead(model::QLearning, s,a) = model.Q[s,a]

function update!(model::QLearning, s,a,r,s_)
    Î³, Q, Î± = model.Î³, model.Q, model.Î±
    Q[s,a] += Î±*(r + maximum(Q[s_, :]) - Q[s,a])  # update of Q-function
    return model 
end 


# SARSA
mutable struct Sarsa
    S
    A
    Î³
    Q  # action value function (initial)
    Î±  # learning rate 
    l  # most recent experience tuble(s,a,r)
end

lookahead(model::Sarsa, s,a) = model.Q[s,a]

function update!(model::Sarsa, s,a,r,s_)
    if model.l != nothing 
        Î³, Q, Î±, l = model.Î³, model.Q, model.Î±, model.l 
        model.Q[l.s, l.a] += Î± * (l.r + Î³*Q[s,a] - Q[l.s, l.a]) 
    end 
    model.l = (s=s, a=a, r=r) 
end 


# Sarsa(Î»)
mutable struct SarsaLambda
    S 
    A 
    Î³
    Q
    N  # trace (visit count)
    Î±  # lr 
    Î»  # trace decay rate 
    l  # most recent experinece tuple (s,a,r)
end

lookahead(model::SarsaLambda, s, a) = model.Q[s,a]

function update!(model::SarsaLambda, s, a, r, s_)
    if model.l != nothing 
        Î³, Î», Q, Î±, l = model.Î³, model.Î», model.Q, model.Î±, model.l 
        model.N[l.s, l.a] += 1 
        Î´ = l.r + Î³ * Q[s,a] - Q[l.s, l.a]  # temporal differece update
        for s in model.S
            for a in model.A
                model.Q[s,a] += Î±*Î´*model.N[s,a]
                model.N[s,a] *= Î³*Î»  # decay visit counts 
            end 
        end 
    else 
        model.N[:,:] = 0  # initialize the visit vount 
    end 
    model.l = (s=s, a=a, r=r)
    return model
end 


# Gradient Q-learning (continuous space) -> can we connect to the DQN with this? 
struct GradientQLearning 
    # S is continuous so not defined here
    A 
    Î³
    Q 
    âˆ‡Q   # gradient dQ/dÎ¸ 
    Î¸    # parameter for the Q function 
    Î±    # learning rate 
end 

function lookahead(model::GradientQLearning, s, a)
    return model.Q(model.Î¸, s, a)
end 

function update!(model::GradientQLearning, s,a,r,s_)
    A, Î³, Q, Î¸, Î± = model.A, model.Î³, model.Q, model.Î¸, model.Î±
    Qvec = [Q(Î¸, s_, a_) for a_ in A]
    # println("Qvec: ", Qvec)
    
    u = maximum(Qvec)
    # println("u: ", u)
    # println("Q(Î¸,s,a): ", Q(Î¸, s, a))
    # println("âˆ‡Q: ", model.âˆ‡Q(Î¸,s,a))
    Î” = (r + Î³*u - Q(Î¸, s, a)) * model.âˆ‡Q(Î¸,s,a)
    # println("Î”: ", Î”)
    Î¸[:] += Î± * scale_gradient(Î”, 1)   # scale gradient to prevent too catastrophically big gradient 
    return model 
end 

# gradient Q learning with Experince Replay, "batch" learning
struct ReplayGradientQLearning 
    # S is continuous so not defined here
    A 
    Î³
    Q 
    âˆ‡Q   # gradient dQ/dÎ¸ 
    Î¸    # parameter for the Q function 
    Î±    # learning rate
    buffer  # circular memory buffer
    m       # number of samples to be discarded at the gradient updates 
    m_grad  # batch size 
end 

function lookahead(model::ReplayGradientQLearning, s, a)
    return model.Q(model.Î¸, s, a)
end  

function update!(model::ReplayGradientQLearning, s,a,r,s_)
    A, Î³, Q, Î¸, Î± = model.A, model.Î³, model.Q, model.Î¸, model.Î±
    buffer, m, m_grad = model.buffer, model.m, model.m_grad
    
    if isfull(buffer)
        U(s) = maximum(Q(Î¸,s,a) for a in A)
        âˆ‡Q(s,a,r,s_) = (r + r*U(s_) - Q(Î¸,s,a)) * model.âˆ‡Q(Î¸,s,a)
        Î” = mean(âˆ‡Q(s,a,r,s_) for (s,a,r,s_) in rand(bugger, m_grad))
        Î¸[:] += Î± * scale_gradient(Î”, 1)
        for i in 1:m 
            popfirst!(buffer)  # discard the experience 
        end
    else 
        push!(buffer, (s,a,r,s_))
    end 
    return model 
end 


""" Training """

# algorithm 15.9
function train_online(ğ’«::MDP, model, Ï€, h, s) 
    for i in 1:h 
        a = Ï€(model, s) 
        sâ€², r = ğ’«.TR(s, a) 
        update!(model, s, a, r, sâ€²)  # update model from the sample (s,a,r,s_)
        s = sâ€² 
    end 
end

function train_offline(ğ’«::MDP, model, df, h) 
    for i in 1:h 
        s, a, r, s_ = sample_data(model, df)   # TODO: add Ïµ after implementing the Ïµ-greedy in sample_data()
        update!(model, s, a, r, s_)  # update model from the sample (s,a,r,s_)
    end 
end


""" Prevention of gradient overshooting """

scale_gradient(âˆ‡, L2_max) = min(L2_max/norm(âˆ‡), 1)*âˆ‡ 
clip_gradient(âˆ‡, a, b)    = clamp.(âˆ‡, a, b)


"""  Sampling technique """
# for offline RL, sample a tuple (s,a,r,s_) to update a model
function sample_data(model, df, Ïµ=nothing)
    row = size(df,1)
    if Ïµ != nothing   # Ïµ-greedy? 
        if rand() < Ïµ 
            i = rand(1:row)
            s, a, r, s_ = df.s[i], df.a[i], df.r[i], df.sp[i] 
        else 
            # TODO: greedy Q function search (discrete: look up Q table's row / continuous: ??)
            # how to define the initial state if so? 
            Qtable = model.Q

        end 
    else  # random sampling 
        i = rand(1:row)
        s, a, r, s_ = [df.s_i[i], df.s_j[i]], df.a[i], df.r[i], [df.sp_i[i], df.sp_j[i]] 
        # s, a, r, s_ = df.s[i], df.a[i], df.r[i], df.sp[i] 
    end
        return s, a, r, s_
end 


# Ïµ-greedy 
struct EpsilonGreedyExploration
    Ïµ # probability of random arm
end
   
function (Ï€::EpsilonGreedyExploration)(model, s)   # FIXME: this is for online stragety, we need offline version of this
    A, Ïµ = Ï€.A, Ï€.Ïµ
    if rand() < Ïµ
        return rand(A)  # exploration  # FIXME: random sampling from the data 
    end 
    Q(s,a) = lookahead(model, s, a)
    return argmax(a->Q(s,a), A)   # greedy (exploitation)  # FIXME: find the max Q 
end

