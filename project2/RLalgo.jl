using Printf
using DataFrames
using CSV
using LinearAlgebra


struct MDP 
    Œ≥   # discount rate 
    S   # state space
    A   # action space
    T   # transition function
    R   # reward function
    TR  # transition function 
end 


""" Update scheme """

# Incremental Estimate (expectation)
mutable struct IncrementalEstimate
    Œº  # mean estimate 
    Œ±  # learning rate (function) 
    m  # num of updates 
end 

function update!(model::IncrementalEstimate, x)
    model.m += 1 
    model.Œº += model.Œ±(model.m) * (x - model.Œº) 
    return model 
end 

# Q-learning 
mutable struct QLearning
    S 
    A
    Œ≥
    Q
    Œ±
end 

lookahead(model::QLearning, s,a) = model.Q[s,a]

function update!(model::QLearning, s,a,r,s_)
    Œ≥, Q, Œ± = model.Œ≥, model.Q, model.Œ±
    Q[s,a] += Œ±*(r + maximum(Q[s_, :]) - Q[s,a])  # update of Q-function
    return model 
end 


# SARSA
mutable struct Sarsa
    S
    A
    Œ≥
    Q  # action value function (initial)
    Œ±  # learning rate 
    l  # most recent experience tuble(s,a,r)
end

lookahead(model::Sarsa, s,a) = model.Q[s,a]

function update!(model::Sarsa, s,a,r,s_)
    if model.l != nothing 
        Œ≥, Q, Œ±, l = model.Œ≥, model.Q, model.Œ±, model.l 
        model.Q[l.s, l.a] += Œ± * (l.r + Œ≥*Q[s,a] - Q[l.s, l.a]) 
    end 
    model.l = (s=s, a=a, r=r, s_=s_) 
end 


# Sarsa(Œª)
mutable struct SarsaLambda
    S 
    A 
    Œ≥
    Q
    N  # trace (visit count)
    Œ±  # lr 
    Œª  # trace decay rate 
    l  # most recent experinece tuple (s,a,r)
end

lookahead(model::SarsaLambda, s, a) = model.Q[s,a]

function update!(model::SarsaLambda, s, a, r, s_)
    if !isnothing(model.l)  
        Œ≥, Œª, Q, Œ±, l = model.Œ≥, model.Œª, model.Q, model.Œ±, model.l 
        # println(l.s, " ", l.a)
        model.N[l.s, l.a] += 1 
        Œ¥ = l.r + Œ≥ * Q[s,a] - Q[l.s, l.a]  # temporal differece update
        for s in model.S
            for a in model.A
                model.Q[s,a] += Œ±*Œ¥*model.N[s,a]
                model.N[s,a] *= Œ≥*Œª  # decay visit counts 
            end 
        end 
    else 
        model.N[:,:] .= 0  # initialize the visit vount 
    end 
    model.l = (s=s, a=a, r=r, s_=s_)
    return model
end 


# Gradient Q-learning (continuous space) -> can we connect to the DQN with this? 
mutable struct GradientQLearning 
    # S is continuous so not defined here
    A 
    Œ≥
    Q 
    ‚àáQ   # gradient dQ/dŒ∏ 
    Œ∏    # parameter for the Q function 
    Œ±    # learning rate 
end 

function lookahead(model::GradientQLearning, s, a)
    return model.Q(model.Œ∏, s, a)
end 

function update!(model::GradientQLearning, s,a,r,s_, i=nothing, h=nothing)
    A, Œ≥, Q, Œ∏, Œ± = model.A, model.Œ≥, model.Q, model.Œ∏, model.Œ±
    Qvec = [Q(Œ∏, s_, a_) for a_ in A]
    # println("Qvec: ", Qvec)
    
    u = maximum(Qvec)
    # println("u: ", u)
    # println("Q(Œ∏,s,a): ", Q(Œ∏, s, a))
    # println("‚àáQ: ", model.‚àáQ(Œ∏,s,a))
    Œî = (r + Œ≥*u - Q(Œ∏, s, a)) * model.‚àáQ(Œ∏,s,a)
    # println("Œî: ", Œî)
    delta =  Œ± * scale_gradient(Œî, 1)
    println(round.(delta; sigdigits=4))
    Œ∏[:] += delta   # scale gradient to prevent too catastrophically big gradient 
    
    model.Œ± *= 0.999  # learning rate decay
    return model 
end 

# gradient Q learning with Experince Replay, "batch" learning
struct ReplayGradientQLearning 
    # S is continuous so not defined here
    A 
    Œ≥
    Q 
    ‚àáQ   # gradient dQ/dŒ∏ 
    Œ∏    # parameter for the Q function 
    Œ±    # learning rate
    buffer  # circular memory buffer
    m       # number of samples to be discarded at the gradient updates 
    m_grad  # batch size 
end 

function lookahead(model::ReplayGradientQLearning, s, a)
    return model.Q(model.Œ∏, s, a)
end  

function update!(model::ReplayGradientQLearning, s,a,r,s_)
    A, Œ≥, Q, Œ∏, Œ± = model.A, model.Œ≥, model.Q, model.Œ∏, model.Œ±
    buffer, m, m_grad = model.buffer, model.m, model.m_grad
    
    if isfull(buffer)
        U(s) = maximum(Q(Œ∏,s,a) for a in A)
        ‚àáQ(s,a,r,s_) = (r + r*U(s_) - Q(Œ∏,s,a)) * model.‚àáQ(Œ∏,s,a)
        Œî = mean(‚àáQ(s,a,r,s_) for (s,a,r,s_) in rand(bugger, m_grad))
        Œ∏[:] += Œ± * scale_gradient(Œî, 1)
        for i in 1:m 
            popfirst!(buffer)  # discard the experience 
        end
    else 
        push!(buffer, (s,a,r,s_))
    end 
    return model 
end 


""" Training """

# algorithm 15.9
function train_online(ùí´::MDP, model, œÄ, h, s) 
    for i in 1:h 
        a = œÄ(model, s) 
        s‚Ä≤, r = ùí´.TR(s, a) 
        update!(model, s, a, r, s‚Ä≤)  # update model from the sample (s,a,r,s_)
        s = s‚Ä≤ 
    end 
end

function train_offline(ùí´::MDP, model::SarsaLambda, df, h) 
    for i in 1:h 
        s, a, r, s_ = sample_data(model, df)   # TODO: add œµ after implementing the œµ-greedy in sample_data()
        update!(model, s, a, r, s_)  # update model from the sample (s,a,r,s_)
    end 
end


function train_offline(ùí´::MDP, model::GradientQLearning, df, h) 
    for i in 1:h 
        s, a, r, s_ = sample_data(model, df)   # TODO: add œµ after implementing the œµ-greedy in sample_data()
        update!(model, s, a, r, s_, i, h)  # update model from the sample (s,a,r,s_)
    end 
end




""" Prevention of gradient overshooting """

scale_gradient(‚àá, L2_max) = min(L2_max/norm(‚àá), 1)*‚àá 
clip_gradient(‚àá, a, b)    = clamp.(‚àá, a, b)


"""  Sampling technique """
# for offline RL, sample a tuple (s,a,r,s_) to update a model
function sample_data(model::GradientQLearning, df, œµ1=nothing, œµ2 = 0.2)
    row = size(df,1)
    if !isnothing(œµ1)   # œµ-greedy? 
        if rand() < œµ1 
            i = rand(1:row)
            s, a, r, s_ = df.s[i], df.a[i], df.r[i], df.sp[i] 
        else 
            # TODO: greedy Q function search (discrete: look up Q table's row / continuous: ??)
            # how to define the initial state if so? 
            Qtable = model.Q

        end 
    else  # random sampling with reward bias 
        # println("random sampling")
        if rand() < œµ2
            idx = findall(df.r .> 0)  # find all the positive rewards
            i = rand(idx) 
        else
            i = rand(1:row)
        end 
        # s, a, r, s_ = [df.s_i[i], df.s_j[i]], df.a[i], df.r[i], [df.sp_i[i], df.sp_j[i]]  # small.CSV
        s, a, r, s_ = [df.pos[i], df.vel[i]], df.a[i], df.r[i], [df.pos_[i], df.vel_[i]]  # medium.CSV
        
        # println("sampled: ", s, " ", a, " ", r, " ", s_)
        # s, a, r, s_ = df.s[i], df.a[i], df.r[i], df.sp[i] 
    end
        return s, a, r, s_
end 

# for offline RL, sample a tuple (s,a,r,s_) to update a model
function sample_data(model::SarsaLambda, df, œµ1=nothing, œµ2 = 0.3)
    row = size(df,1)
    if !isnothing(œµ1)   # œµ-greedy? 
        if rand() < œµ1 
            i = rand(1:row)
            s, a, r, s_ = df.s[i], df.a[i], df.r[i], df.sp[i] 
        else 
            # TODO: greedy Q function search (discrete: look up Q table's row / continuous: ??)
            # how to define the initial state if so? 
            Qtable = model.Q

        end 
    else  # random sampling with reward bias 
        # println("random sampling")
        l = model.l

        if !isnothing(l)
            s,a,r,s_ = l.s, l.a, l.r, l.s_
            # s_ = gridworld_step(s,a)
        else 
            s_ = NaN  
        end 

        if isnan(s_)
            # random sampling 
            if rand() < œµ2
                idx = findall(df.r .> 0)  # find all the positive rewards
                i = rand(idx) 
            else
                i = rand(1:row)
            end 
        else 
            # find the index of the succeeding sample 
            idx = findall(df.s .== s_) 
            i = rand(idx)
        end
        
        # s, a, r, s_ = [df.s_i[i], df.s_j[i]], df.a[i], df.r[i], [df.sp_i[i], df.sp_j[i]] 
        s, a, r, s_ = df.s[i], df.a[i], df.r[i], df.sp[i] 
        # println("sampled: ", s, " ", a, " ", r, " ", s_)

    end
    
    return s, a, r, s_
end 


# œµ-greedy 
struct EpsilonGreedyExploration
    œµ # probability of random arm
end
   
function (œÄ::EpsilonGreedyExploration)(model, s)   # FIXME: this is for online stragety, we need offline version of this
    A, œµ = œÄ.A, œÄ.œµ
    if rand() < œµ
        return rand(A)  # exploration  # FIXME: random sampling from the data 
    end 
    Q(s,a) = lookahead(model, s, a)
    return argmax(a->Q(s,a), A)   # greedy (exploitation)  # FIXME: find the max Q 
end



""" Gridworld """
function gridworld_step(s::Int,a::Int)

    if a == 1 # left 
        if s <= 10 
            s_ = NaN
        else
            s_ = s - 10 
        end
    elseif a == 2 # right
        if s >= 91 
            s_ = NaN
        else
            s_ = s + 10
        end
    elseif a == 3 # up
        if mod(s,10) == 1
            s_ = NaN 
        else 
            s_ = s - 1 
        end 
    elseif a == 4 # down
        if mod(s,10) == 0 
            s_ = NaN
        else
            s_ = s + 1
        end
    end

    return s_
end 


""" Medium """

function extract_state(s)
    vel = ( s - 1 ) √∑ 500
    pos = ( s - 1 ) % 500
    return [pos, vel] 
end 

function pack_state(pos,vel)
    return 1 + pos + 500*vel
end 

